{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d52b9a-03a7-464f-9869-a80133d97138",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Enviornment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64daa283-b7a6-4cd8-88b7-d0941248161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = \"C:/Users/Dell/Spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_HOME'] = \"C:/Users/Dell/Hadoop/\" \n",
    "# HADOOP_HOME AND put hadoop.dll file into the C:/Windows/System32 folder solved the IO operation error.\n",
    "# os.environ['PATH'] = \"C:/Users/Dell/Hadoop/bin/\" NOT REQUIRED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d419d-5e88-4b3d-80a8-dff4e898e579",
   "metadata": {},
   "source": [
    "# Getting started with SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175039da-e21f-4327-ba8f-911c1b97827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Get-Started\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e434eb-6d3c-47c5-a2b8-59910744d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the setup\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2f129-1411-40bc-a6db-ffe1b95c0499",
   "metadata": {},
   "source": [
    "# Create SparkContext in Apache Spark version 2.x and later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a41dbc6-61c6-474c-aa3d-45bf8fc9e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApplication\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fead832-d7ca-43f0-af66-9cfef5ecea8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CBMR8U5.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark-Get-Started</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySpark-Get-Started>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac8592-786f-4bc9-92e5-cfd508e5cb19",
   "metadata": {},
   "source": [
    "# RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c177b7e8-fe38-4199-b8b8-3f1542f002b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85e0b50-d95f-400b-8453-20fcab68369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD-Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863ea7ee-e741-4ac8-8d77-58f283e6cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of tuples\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35), (\"Alice\", 40)]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424b0f2-c5d8-4e82-b945-64d0dc1fb8f3",
   "metadata": {},
   "source": [
    "## RDDs Operation: Actions (Eager Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2389b4f-1dd8-4c28-a350-cb6b113e024d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of elements in rdd:  4\n"
     ]
    }
   ],
   "source": [
    "# Count action: Count the number of elements in the RDD\n",
    "count = rdd.count()\n",
    "print(\"The total number of elements in rdd: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8375799-5e99-4004-bbbb-dc4b4ad726a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of the rdd:  ('Alice', 25)\n"
     ]
    }
   ],
   "source": [
    "# First action: Retrieve the first element of the RDD\n",
    "first_element = rdd.first()\n",
    "print(\"The first element of the rdd: \", first_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934977ec-b1ca-4bcb-a0a0-d6b0a12bb5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two elements of the rdd:  [('Alice', 25), ('Bob', 30)]\n"
     ]
    }
   ],
   "source": [
    "# Take action: Retrieve the n elements of the RDD\n",
    "taken_elements = rdd.take(2)\n",
    "print(\"The first two elements of the rdd: \", taken_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96713545-84c3-4752-af3f-54bb47a885b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreach action: Print each element of the RDD\n",
    "rdd.foreach(lambda x: print(x))\n",
    "\n",
    "# This will not print on console. https://stackoverflow.com/questions/28804647/why-does-foreach-not-bring-anything-to-the-driver-program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eeeb11-d457-4911-9e87-87381889dc7c",
   "metadata": {},
   "source": [
    "## RDDs Operation: Transformations (Lazy Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9473d8f-b383-4e10-96a6-f3e09dbc1531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd with uppercease name:  [('ALICE', 25), ('BOB', 30), ('CHARLIE', 35), ('ALICE', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mapped_rdd = rdd.map(lambda x: (x[0].upper(), x[1]))\n",
    "\n",
    "result = mapped_rdd.collect()\n",
    "print(\"rdd with uppercease name: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4977d50a-88b3-4ec7-9ed9-721509cf6664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 35), ('Alice', 40)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter transformation: Filter records where age is greater than 30\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 30)\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a67b56b6-aefd-433a-a1b0-7733876c1dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 35), ('Alice', 65), ('Bob', 30)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReduceByKey transformation: Calculate the total age for each name\n",
    "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f22d121-f68e-4cb5-b1a3-b08553826897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 40), ('Charlie', 35), ('Bob', 30), ('Alice', 25)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SortBy transformation: Sort the RDD by age in descending order\n",
    "sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450753b-c7f3-4552-916c-872e1a7b9038",
   "metadata": {},
   "source": [
    "## Save RDDs to text file and read RDDs from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1923e23b-4580-4690-b793-e92b847468a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save action: Save the RDD to a text file\n",
    "rdd.saveAsTextFile(\"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed52bdc-e02a-4384-8805-51465f49a52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('Alice', 25)\", \"('Bob', 30)\", \"('Charlie', 35)\", \"('Alice', 40)\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create rdd from text file\n",
    "rdd_text = spark.sparkContext.textFile(\"output.txt\")\n",
    "rdd_text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a924fe3-4943-4a37-83a5-d4635b60be91",
   "metadata": {},
   "source": [
    "## Shut down Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8987d4f4-0467-4b39-9ed5-9946d13e43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eecb6e-7b30-4e9e-acf7-73cbcd8b689b",
   "metadata": {},
   "source": [
    "# Dataframes Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbdd2e04-d0a9-4d90-a303-16bbc9d58521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19284606-f7ff-477b-899d-53e296bc4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame-Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6eb601-e7e2-47b7-8e99-c5d6693bf730",
   "metadata": {},
   "source": [
    "## Using RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7f2542-276a-4f35-ad2a-fe3608cd1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"./data/data.txt\")\n",
    "result_rdd = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e345235f-a236-4068-a665-9d504252c442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 12),\n",
       " ('of', 7),\n",
       " ('a', 7),\n",
       " ('distributed', 5),\n",
       " ('in', 5),\n",
       " ('Spark', 4),\n",
       " ('as', 3),\n",
       " ('is', 3),\n",
       " ('API', 3),\n",
       " ('on', 3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59990159-5472-46a5-98e4-494f006dc9f7",
   "metadata": {},
   "source": [
    "## Using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e40fbea-8e7b-4686-92b0-e9cd2f10832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"./data/data.txt\")\n",
    "\n",
    "result_df = df.selectExpr(\"explode(split(value, ' ')) as word\") \\\n",
    "    .groupBy(\"word\").count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce3b172e-a7fc-4841-9aaa-af9cd6402e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='the', count=12),\n",
       " Row(word='of', count=7),\n",
       " Row(word='a', count=7),\n",
       " Row(word='in', count=5),\n",
       " Row(word='distributed', count=5),\n",
       " Row(word='Spark', count=4),\n",
       " Row(word='API', count=3),\n",
       " Row(word='RDD', count=3),\n",
       " Row(word='is', count=3),\n",
       " Row(word='on', count=3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aca2073-dcaa-4244-8a05-0fd15d45fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8a28f-c93c-46ec-8afc-9ef6ee63bccb",
   "metadata": {},
   "source": [
    "# DataFrame - various data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25f259f5-2691-42ed-8f1f-2aa489d202c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Create-DataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b2bad-a984-4d3d-bb55-2a66700caa2e",
   "metadata": {},
   "source": [
    "## Read CSV file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdaf1ac7-6644-4df7-993f-385ab24c77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d96bd2fd-298a-4849-a1f9-4928f013ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f8e56-879e-474a-9639-376fdca8da1e",
   "metadata": {},
   "source": [
    "## Read CSV with an explicit schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d307e67c-843a-48e1-81d9-30bb776b1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a965fdd3-b5ee-486b-8095-f46ff68309f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"category\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"quantity\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"price\", dataType=DoubleType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "267df9a5-d0e1-4cee-bd08-839e898390ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with schema definition\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b0e7eb9-e621-4646-8225-b0203b529e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529bf81d-a1c3-475f-9d73-97f8a74c6362",
   "metadata": {},
   "source": [
    "## Read CSV with inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "979b46df-e222-42bb-ba9a-273e557082eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with inferSchema\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21a582b1-c5f1-45ba-bf6c-947b2740f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa73e1-cf35-4907-814b-9f08af6a60f8",
   "metadata": {},
   "source": [
    "## Read JSON file into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305d0b4-040c-40ba-b4b7-d7181e03205e",
   "metadata": {},
   "source": [
    "### Single Line json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77c949eb-7024-4fba-898e-2828943c463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read single line JSON\n",
    "# Each row is a JSON record, records are separated by new line\n",
    "json_file_path = \"./data/products_singleline.json\"\n",
    "df = spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "830d78b5-0060-49f9-ab43-c0fb75e616eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d81b3e-c3c4-427f-a0e1-5470a92e6fe6",
   "metadata": {},
   "source": [
    "### Multi-Line json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72d1a22f-76a3-4aa7-bc7b-3ddb0a6da144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multi-line JSON\n",
    "# JSON is an array of record, records are separated by a comma.\n",
    "# each record is defined in multiple lines\n",
    "json_file_path = \"./data/products_multiline.json\"\n",
    "df = spark.read.json(json_file_path, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69daf841-50bb-4b47-bca4-47799daa3085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c49396-86c5-4d85-aa8f-f510cee7153d",
   "metadata": {},
   "source": [
    "## Read parquet file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72d1b7cb-72d1-4b1c-a4a9-9a5bdb41cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataframe into parquet file\n",
    "parquet_file_path = \"./data/products.parquet\"\n",
    "df = spark.read.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dceae390-b407-43e0-9737-6f5ee82113be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2640f05-08d4-41ad-a592-c16e26250285",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58aba87-6499-4d65-bb9a-d5598c1dd4f1",
   "metadata": {},
   "source": [
    "# DataFrame - Various data operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a213b4c-cb3b-4308-b9a1-2151db3f3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame-Operations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0229425c-14e0-4cec-afb4-d70777f25288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/stocks.txt\"\n",
    "df = spark.read.csv(data_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f336167-b2c6-44a5-8617-66e7d8597349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+----------+-----------+--------+-------+\n",
      "| id|      name|   category|quantity|  price|\n",
      "+---+----------+-----------+--------+-------+\n",
      "|  1|    iPhone|Electronics|      10| 899.99|\n",
      "|  2|   Macbook|Electronics|       5|1299.99|\n",
      "|  3|      iPad|Electronics|      15| 499.99|\n",
      "|  4|Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|     LG TV|Electronics|      10| 699.99|\n",
      "+---+----------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0d947f3-21ad-486f-84bc-ba8bbf1442fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Columns:\n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "selected_columns = df.select(\"id\", \"name\", \"price\")\n",
    "print(\"Selected Columns:\")\n",
    "selected_columns.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90921cd1-a901-4003-a5bf-3d27b2900292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data: 12\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| id|          name|   category|quantity|price|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "|  6|    Nike Shoes|   Clothing|      30|99.99|\n",
      "|  7|  Adidas Shoes|   Clothing|      25|89.99|\n",
      "| 12|        Apples|       Food|     100|  0.5|\n",
      "| 13|       Bananas|       Food|     150| 0.25|\n",
      "| 14|       Oranges|       Food|     120| 0.75|\n",
      "| 15|Chicken Breast|       Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|       Food|      30| 5.99|\n",
      "| 24|    Laptop Bag|Accessories|      25|29.99|\n",
      "| 25|      Backpack|Accessories|      30|24.99|\n",
      "| 28|         Jeans|   Clothing|      30|59.99|\n",
      "| 29|       T-shirt|   Clothing|      50|14.99|\n",
      "| 30|      Sneakers|   Clothing|      40|79.99|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered_data = df.filter(df.quantity > 20)\n",
    "print(\"Filtered Data:\", filtered_data.count())\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8ed2a0d-6e77-4a93-925c-8c287205e404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped and Aggregated Data:\n",
      "+-----------+-------------+------------------+\n",
      "|   category|sum(quantity)|        avg(price)|\n",
      "+-----------+-------------+------------------+\n",
      "|       Food|          450|2.2960000000000003|\n",
      "|     Sports|           35|             34.99|\n",
      "|Electronics|           98| 586.6566666666665|\n",
      "|   Clothing|          200|  99.2757142857143|\n",
      "|  Furniture|           41|            141.99|\n",
      "|Accessories|           55|             27.49|\n",
      "+-----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Aggregations\n",
    "grouped_data = df.groupBy(\"category\").agg({\"quantity\": \"sum\", \"price\": \"avg\"})\n",
    "print(\"Grouped and Aggregated Data:\")\n",
    "grouped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "859af957-00a0-48c1-9a32-c358b38d302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Data:\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "| id|            name|   category|quantity|  price|   category|\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|Electronics|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|Electronics|\n",
      "|  3|            iPad|Electronics|      15| 499.99|Electronics|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|Electronics|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|Electronics|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|   Clothing|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|   Clothing|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|Electronics|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|Electronics|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|  Furniture|\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join with another DataFrame\n",
    "df2 = df.select(\"id\", \"category\").limit(10)\n",
    "joined_data = df.join(df2, \"id\", \"inner\")\n",
    "print(\"Joined Data:\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e609793-13c2-473f-b84e-596aae470f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Data:\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| id|          name|   category|quantity|price|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| 13|       Bananas|       Food|     150| 0.25|\n",
      "| 12|        Apples|       Food|     100|  0.5|\n",
      "| 14|       Oranges|       Food|     120| 0.75|\n",
      "| 15|Chicken Breast|       Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|       Food|      30| 5.99|\n",
      "| 29|       T-shirt|   Clothing|      50|14.99|\n",
      "| 19|      Yoga Mat|     Sports|      20|19.99|\n",
      "| 25|      Backpack|Accessories|      30|24.99|\n",
      "| 24|    Laptop Bag|Accessories|      25|29.99|\n",
      "| 20|  Dumbbell Set|     Sports|      15|49.99|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column\n",
    "sorted_data = df.orderBy(\"price\")\n",
    "print(\"Sorted Data:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "686a8d39-7197-48ad-a94e-6c44416f5359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Data Descending:\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "| 26|          Camera|Electronics|      10| 599.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "| 17|  Leather Jacket|   Clothing|      15| 199.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 18|     Winter Coat|   Clothing|      10| 149.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column desc\n",
    "from pyspark.sql.functions import col, desc\n",
    "sorted_data = df.orderBy(col(\"price\").desc(), col(\"id\").desc())\n",
    "print(\"Sorted Data Descending:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cdfb9c9-7cf9-41ba-bb24-a30a7d9d2cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Product Categories:\n",
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|       Food|\n",
      "|     Sports|\n",
      "|Electronics|\n",
      "|   Clothing|\n",
      "|  Furniture|\n",
      "|Accessories|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get distinct product category\n",
    "distinct_rows = df.select(\"category\").distinct()\n",
    "print(\"Distinct Product Categories:\")\n",
    "distinct_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86e21562-5992-497a-a8a8-e0243f559f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped Columns:\n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns\n",
    "dropped_columns = df.drop(\"quantity\", \"category\")\n",
    "print(\"Dropped Columns:\")\n",
    "dropped_columns.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3053a802-31ac-4e5e-b03e-cc294484cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with New Column:\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "| id|            name|   category|quantity|  price|revenue|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99| 8999.9|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|6499.95|\n",
      "|  3|            iPad|Electronics|      15| 499.99|7499.85|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|6399.92|\n",
      "|  5|           LG TV|Electronics|      10| 699.99| 6999.9|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99| 2999.7|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|2249.75|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|1799.88|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99| 3999.8|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99| 2499.9|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new calculated column\n",
    "df_with_new_column = df.withColumn(\"revenue\", df.quantity * df.price)\n",
    "print(\"DataFrame with New Column:\")\n",
    "df_with_new_column.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e7822f6-3211-4a29-be78-b6229d191aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Aliased Column:\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "| id|            name|   category|quantity|product_price|\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "|  1|          iPhone|Electronics|      10|       899.99|\n",
      "|  2|         Macbook|Electronics|       5|      1299.99|\n",
      "|  3|            iPad|Electronics|      15|       499.99|\n",
      "|  4|      Samsung TV|Electronics|       8|       799.99|\n",
      "|  5|           LG TV|Electronics|      10|       699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|        99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|        89.99|\n",
      "|  8| Sony Headphones|Electronics|      12|       149.99|\n",
      "|  9|Beats Headphones|Electronics|      20|       199.99|\n",
      "| 10|    Dining Table|  Furniture|      10|       249.99|\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns using alias\n",
    "df_with_alias = df.withColumnRenamed(\"price\", \"product_price\")\n",
    "print(\"DataFrame with Aliased Column:\")\n",
    "df_with_alias.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de70414b-9048-48b0-bc1f-0f100275b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b093d-99a2-423d-baeb-02c24a84b24c",
   "metadata": {},
   "source": [
    "# Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aeafae7b-b9ea-4c4d-a727-fe81ca71374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b2fd5e3-895d-467d-b764-693b57e292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/persons.csv\"\n",
    "df = spark.read.csv(data_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca6c91d4-582f-4df4-8041-635703d8c893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "Initial DataFrame:\n",
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|        Jane Smith| 25|Female| 45000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "+------------------+---+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show the initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acf9fae7-ea96-432c-b897-47bbb3c3b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a Temporary Table\n",
    "df.createOrReplaceTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "452bfc69-3a14-4d80-833f-d01e3538a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all rows where age is greater than 25\n",
    "result = spark.sql(\"SELECT * FROM my_table WHERE age > 25\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc4e4638-006d-4882-bce7-77f45ee135bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|gender|avg_salary|\n",
      "+------+----------+\n",
      "|Female|   52300.0|\n",
      "|  Male|   62100.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the average salary by gender\n",
    "avg_salary_by_gender = spark.sql(\"SELECT gender, AVG(salary) as avg_salary FROM my_table GROUP BY gender\")\n",
    "avg_salary_by_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99fb3481-5ba6-42a0-922d-94319c08c712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop a view\n",
    "spark.catalog.dropTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bf0f6ca-e9cb-45e3-9337-47a193de377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ed0effe-724d-4083-b7d5-39f32b1c65da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the temporary view\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 25\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a117e89-1aba-4c60-b3f4-7a3e8d20af40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a temporary view exists\n",
    "view_exists = spark.catalog.tableExists(\"people\")\n",
    "view_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cb469a0-7276-4e16-b730-7341ad07eea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop a temporary view\n",
    "spark.catalog.dropTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97c7f156-9c6c-41cb-9ecd-a5c0a58999e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a temporary view exists\n",
    "view_exists = spark.catalog.tableExists(\"people\")\n",
    "view_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7482497-4a36-468b-a662-19d1a7014a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|   John|\n",
      "|  2|  Alice|\n",
      "|  3|    Bob|\n",
      "|  4|  Emily|\n",
      "|  5|  David|\n",
      "|  6|  Sarah|\n",
      "|  7|Michael|\n",
      "|  8|   Lisa|\n",
      "|  9|William|\n",
      "+---+-------+\n",
      "\n",
      "+----------+---+------+\n",
      "|department| id|salary|\n",
      "+----------+---+------+\n",
      "|        HR|  1| 60000|\n",
      "|        HR|  2| 55000|\n",
      "|        HR|  3| 58000|\n",
      "|        IT|  4| 70000|\n",
      "|        IT|  5| 72000|\n",
      "|        IT|  6| 68000|\n",
      "|     Sales|  7| 75000|\n",
      "|     Sales|  8| 78000|\n",
      "|     Sales|  9| 77000|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames\n",
    "employee_data = [\n",
    "    (1, \"John\"), (2, \"Alice\"), (3, \"Bob\"), (4, \"Emily\"),\n",
    "    (5, \"David\"), (6, \"Sarah\"), (7, \"Michael\"), (8, \"Lisa\"),\n",
    "    (9, \"William\")\n",
    "]\n",
    "employees = spark.createDataFrame(employee_data, [\"id\", \"name\"])\n",
    "\n",
    "salary_data = [\n",
    "    (\"HR\", 1, 60000), (\"HR\", 2, 55000), (\"HR\", 3, 58000),\n",
    "    (\"IT\", 4, 70000), (\"IT\", 5, 72000), (\"IT\", 6, 68000),\n",
    "    (\"Sales\", 7, 75000), (\"Sales\", 8, 78000), (\"Sales\", 9, 77000)\n",
    "]\n",
    "salaries = spark.createDataFrame(salary_data, [\"department\", \"id\", \"salary\"])\n",
    "\n",
    "employees.show()\n",
    "\n",
    "salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3acbb742-b32f-4a32-8ccc-a948c903aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "salaries.createOrReplaceTempView(\"salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8c11835-b079-4626-9ec4-48b6898c084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  Emily|\n",
      "|  David|\n",
      "|Michael|\n",
      "|   Lisa|\n",
      "|William|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subquery to find employees with salaries above average\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name\n",
    "    FROM employees\n",
    "    WHERE id IN (\n",
    "        SELECT id\n",
    "        FROM salaries\n",
    "        WHERE salary > (SELECT AVG(salary) FROM salaries)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8fb8ed36-4a98-455d-b484-5a34146308ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a876ade2-f00b-48d8-8176-9a01c59bc49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+\n",
      "|department| id|salary|   name|\n",
      "+----------+---+------+-------+\n",
      "|        HR|  1| 60000|   John|\n",
      "|        HR|  2| 55000|  Alice|\n",
      "|        HR|  3| 58000|    Bob|\n",
      "|        IT|  4| 70000|  Emily|\n",
      "|        IT|  5| 72000|  David|\n",
      "|        IT|  6| 68000|  Sarah|\n",
      "|     Sales|  7| 75000|Michael|\n",
      "|     Sales|  9| 77000|William|\n",
      "|     Sales|  8| 78000|   Lisa|\n",
      "+----------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_salary = spark.sql(\"\"\"\n",
    "    select  salaries.*, employees.name\n",
    "    from salaries \n",
    "    left join employees on salaries.id = employees.id\n",
    "\"\"\")\n",
    "\n",
    "employee_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "691cac90-df57-4e97-99ba-c66d9315a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db1e3479-7186-4dfc-ae34-de80bdfd0650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+----+\n",
      "|department| id|salary|   name|rank|\n",
      "+----------+---+------+-------+----+\n",
      "|        HR|  1| 60000|   John|   1|\n",
      "|        HR|  3| 58000|    Bob|   2|\n",
      "|        HR|  2| 55000|  Alice|   3|\n",
      "|        IT|  5| 72000|  David|   1|\n",
      "|        IT|  4| 70000|  Emily|   2|\n",
      "|        IT|  6| 68000|  Sarah|   3|\n",
      "|     Sales|  8| 78000|   Lisa|   1|\n",
      "|     Sales|  9| 77000|William|   2|\n",
      "|     Sales|  7| 75000|Michael|   3|\n",
      "+----------+---+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the rank of employees within each department based on salary\n",
    "employee_salary.withColumn(\"rank\", F.rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "134cf250-68ff-403e-9b82-fcbf48e17a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
